{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNPtQeRBB+B8bR5hBmPJxLB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2192c04890ec477b9397833501592553": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_fb81da0f33364323bd4d7b24b51d95a9"
          }
        },
        "c700937d6a3a4dd19f3bdd39d3c9d34d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10fb345a3ce8488296e4c20a5a0fba3e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_aa735c02aedd4b1793c445ac4607f62a",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "7dafc1856b404830a64374bfa34e4e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_cb18a16a0aa8483fa87a3a592ebf234e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_74ed36f01d504bc8be9ff9d29906cc46",
            "value": ""
          }
        },
        "d72d12cfd85947b2bc0ed35c4e6aebb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_ed92560cd5284a55af49e7fa96920bd4",
            "style": "IPY_MODEL_b1913ba64066469a99cd28b4019a0511",
            "value": true
          }
        },
        "bccf15c0ca044ad1bf95a4ad31dcff8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_4fec0b3231d041df808493136b3142e8",
            "style": "IPY_MODEL_e5e416d1c7d945cc9d68c2746f74c405",
            "tooltip": ""
          }
        },
        "dd0e27a721984658b0c7a31f9fc4f118": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f09ca95f0064abe8492e7a94e2b9567",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a0e20ef27d8f4be78d071fc2739ffad3",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "fb81da0f33364323bd4d7b24b51d95a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "10fb345a3ce8488296e4c20a5a0fba3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa735c02aedd4b1793c445ac4607f62a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb18a16a0aa8483fa87a3a592ebf234e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74ed36f01d504bc8be9ff9d29906cc46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed92560cd5284a55af49e7fa96920bd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1913ba64066469a99cd28b4019a0511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fec0b3231d041df808493136b3142e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5e416d1c7d945cc9d68c2746f74c405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "1f09ca95f0064abe8492e7a94e2b9567": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0e20ef27d8f4be78d071fc2739ffad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d990fa0e64d43dfbed351140f0ec122": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d35a2c3599e4d0b9a3a8fc0217efc5c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_cc63d9d6033b43729e71da82c7db4a33",
            "value": "Connecting..."
          }
        },
        "7d35a2c3599e4d0b9a3a8fc0217efc5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc63d9d6033b43729e71da82c7db4a33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prajna1999/eng-or-mt/blob/master/sft_full_mistral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNOn0WCnoMzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd2b1152-d61d-40eb-adaf-7999239e0b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface_hub)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers sentencepiece  huggingface_hub datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "2192c04890ec477b9397833501592553",
            "c700937d6a3a4dd19f3bdd39d3c9d34d",
            "7dafc1856b404830a64374bfa34e4e65",
            "d72d12cfd85947b2bc0ed35c4e6aebb7",
            "bccf15c0ca044ad1bf95a4ad31dcff8e",
            "dd0e27a721984658b0c7a31f9fc4f118",
            "fb81da0f33364323bd4d7b24b51d95a9",
            "10fb345a3ce8488296e4c20a5a0fba3e",
            "aa735c02aedd4b1793c445ac4607f62a",
            "cb18a16a0aa8483fa87a3a592ebf234e",
            "74ed36f01d504bc8be9ff9d29906cc46",
            "ed92560cd5284a55af49e7fa96920bd4",
            "b1913ba64066469a99cd28b4019a0511",
            "4fec0b3231d041df808493136b3142e8",
            "e5e416d1c7d945cc9d68c2746f74c405",
            "1f09ca95f0064abe8492e7a94e2b9567",
            "a0e20ef27d8f4be78d071fc2739ffad3",
            "7d990fa0e64d43dfbed351140f0ec122",
            "7d35a2c3599e4d0b9a3a8fc0217efc5c",
            "cc63d9d6033b43729e71da82c7db4a33"
          ]
        },
        "id": "YOMEfKxgoxJM",
        "outputId": "fcf35b02-0114-4f5d-86a4-b830b32abcfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2192c04890ec477b9397833501592553"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "base_mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", use_fast=False)"
      ],
      "metadata": {
        "id": "HWwII_LymJOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import regex\n",
        "class OdiaTokenizerWrapper:\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_tokenizer,\n",
        "        suffixes=None,\n",
        "        named_entities=None,\n",
        "        compound_roots=None,\n",
        "        force_suffix_split=False,\n",
        "        enable_fuzzy_split=False\n",
        "        ):\n",
        "      self.tokenizer=base_tokenizer\n",
        "      self.vocab=self.tokenizer.get_vocab()\n",
        "\n",
        "      # known Odia suffixes. In future expand the list\n",
        "      self.suffixes=suffixes or[ \"‡¨Æ‡¨æ‡¨®‡≠á\", \"‡¨ó‡≠Å‡¨°‡¨º‡¨ø‡¨ï\", \"‡¨∞‡≠á\", \"‡¨ï‡≠Å\", \"‡¨∞‡≠Å\", \"‡¨ü‡¨ø\", \"‡¨ô‡≠ç‡¨ï\",\"‡¨ô‡≠ç‡¨ï‡≠Å\"]\n",
        "      self.named_entities=named_entities or [\"‡¨ú‡¨ó‡¨®‡≠ç‡¨®‡¨æ‡¨•\", \"‡¨≠‡≠Å‡¨¨‡¨®‡≠á‡¨∂‡≠ç‡≠±‡¨∞\", \"‡¨ï‡¨ü‡¨ï\", \"‡¨™‡≠Å‡¨∞‡≠Ä\", \"‡¨¨‡¨æ‡¨≤‡≠á‡¨∂‡≠ç‡≠±‡¨∞\", \"‡¨∏‡¨Æ‡≠ç‡¨¨‡¨≤‡¨™‡≠Å‡¨∞\",\n",
        "    \"‡¨ì‡¨°‡¨º‡¨ø‡¨∂‡¨æ\", \"‡¨ù‡¨æ‡¨∞‡¨∏‡≠Å‡¨ó‡≠Å‡¨°‡¨º‡¨æ\", \"‡¨ó‡¨û‡≠ç‡¨ú‡¨æ‡¨Æ\", \"‡¨Æ‡≠ü‡≠Ç‡¨∞‡¨≠‡¨û‡≠ç‡¨ú\", \"‡¨®‡≠ü‡¨æ‡¨ó‡¨°‡¨º\", \"‡¨ï‡¨≥‡¨æ‡¨π‡¨æ‡¨£‡≠ç‡¨°‡¨ø\",\n",
        "    \"‡¨ñ‡≠ã‡¨∞‡≠ç‡¨¶‡≠ç‡¨ß‡¨æ\", \"‡¨®‡≠Ç‡¨Ü‡¨™‡¨°‡¨æ\", \"‡¨®‡¨¨‡¨∞‡¨ô‡≠ç‡¨ó‡¨™‡≠Å‡¨∞\", \"‡¨∏‡≠Å‡¨®‡≠ç‡¨¶‡¨∞‡¨ó‡¨°‡¨º\", \"‡¨¨‡¨∞‡¨ó‡¨°‡¨º\", \"‡¨ï‡¨®‡≠ç‡¨ß‡¨Æ‡¨æ‡¨≥\",\n",
        "    \"‡¨ó‡¨û‡≠ç‡¨ú‡≠á‡¨∂‡≠ç‡≠±‡¨∞\", \"‡¨ó‡≠ã‡¨™‡¨æ‡¨≥‡¨™‡≠Å‡¨∞\", \"‡¨™‡¨æ‡¨∞‡¨æ‡¨¶‡≠Ä‡¨™\", \"‡¨Ö‡¨®‡≠Å‡¨ó‡≠Å‡¨≥\", \"‡¨¶‡≠á‡¨¨‡¨ó‡¨°‡¨º\", \"‡¨ï‡≠á‡¨®‡≠ç‡¨¶‡≠Å‡¨ù‡¨∞\",\n",
        "    \"‡¨¨‡≠å‡¨¶‡≠ç‡¨ß\", \"‡¨Æ‡¨≤‡¨ï‡¨æ‡¨®‡¨ó‡¨ø‡¨∞‡¨ø\", \"‡¨ó‡¨£‡≠á‡¨∂\", \"‡¨∂‡¨ø‡¨¨\", \"‡¨≤‡¨ø‡¨ô‡≠ç‡¨ó‡¨∞‡¨æ‡¨ú\", \"‡¨Æ‡¨æ‡¨ì\", \"‡¨Æ‡¨π‡¨æ‡¨®‡¨¶‡≠Ä\",\n",
        "    \"‡¨ö‡¨ø‡¨≤‡¨ø‡¨ï‡¨æ\", \"‡¨∞‡¨•‡¨Ø‡¨æ‡¨§‡≠ç‡¨∞‡¨æ\", \"‡¨ï‡≠ã‡¨£‡¨æ‡¨∞‡≠ç‡¨ï\", \"‡¨∏‡≠Å‡¨∞‡≠ç‡¨Ø‡≠ç‡≠ü‡¨Æ‡¨®‡≠ç‡¨¶‡¨ø‡¨∞\", \"‡¨ß‡≠Ç‡¨≥‡¨ø‡¨ó‡¨ø‡¨∞‡¨ø\", \"‡¨®‡¨®‡≠ç‡¨¶‡¨®‡¨ï‡¨æ‡¨®‡¨®\",\n",
        "    \"‡¨ó‡¨£‡≠á‡¨∂‡¨™‡≠Ç‡¨ú‡¨æ\", \"‡¨¶‡≠Å‡¨∞‡≠ç‡¨ó‡¨æ‡¨™‡≠Ç‡¨ú‡¨æ\", \"‡¨ï‡¨æ‡¨≥‡≠Ä‡¨™‡≠Ç‡¨ú‡¨æ\", \"‡¨≤‡¨ï‡≠ç‡¨∑‡≠ç‡¨Æ‡≠Ä‡¨™‡≠Ç‡¨ú‡¨æ\", \"‡¨∏‡¨∞‡¨∏‡≠ç‡≠±‡¨§‡≠Ä‡¨™‡≠Ç‡¨ú‡¨æ\",\n",
        "    \"‡¨™‡≠ç‡¨∞‡¨≠‡¨æ‡¨§‡≠Ä\", \"‡¨Æ‡¨π‡¨æ‡¨™‡≠ç‡¨∞‡¨≠‡≠Å\", \"‡¨Æ‡¨æ‡¨Å‡¨§‡¨∞‡¨ø‡¨£‡≠Ä\", \"‡¨Æ‡¨æ‡¨Å‡¨∏‡¨æ‡¨Æ‡¨≤‡≠á‡¨∂‡≠ç‡≠±‡¨∞‡≠Ä\", \"‡¨Æ‡¨æ‡¨Å‡¨Æ‡¨£‡¨ø‡¨ï‡≠á‡¨∂‡≠ç‡≠±‡¨∞‡≠Ä\",\n",
        "    \"‡¨Æ‡¨æ‡¨Å‡¨ï‡¨æ‡¨®‡¨ï‡¨¶‡≠Å‡¨∞‡≠ç‡¨ó‡¨æ\", \"‡¨¨‡¨ø‡¨Æ‡¨≥‡¨æ\", \"‡¨≠‡¨æ‡¨ó‡¨¨‡¨§\", \"‡¨ó‡≠Ä‡¨§‡¨æ\", \"‡¨∞‡¨æ‡¨Æ‡¨æ‡≠ü‡¨£\", \"‡¨Æ‡¨π‡¨æ‡¨≠‡¨æ‡¨∞‡¨§\",\n",
        "    \"‡¨ó‡≠ã‡¨™‡¨¨‡¨®‡≠ç‡¨ß‡≠Å\", \"‡¨¨‡¨ø‡¨ú‡≠Å‡¨™‡¨ü‡≠ç‡¨®‡¨æ‡≠ü‡¨ï\", \"‡¨®‡¨¨‡≠Ä‡¨®‡¨™‡¨ü‡≠ç‡¨®‡¨æ‡≠ü‡¨ï\", \"‡¨π‡¨∞‡¨ø‡¨π‡¨∞\", \"‡¨¨‡¨ø‡¨∞‡¨æ‡¨ú‡¨æ\", \"‡¨ö‡¨®‡≠ç‡¨¶‡≠ç‡¨∞‡¨≠‡¨æ‡¨ó‡¨æ\",\n",
        "    \"‡¨Æ‡¨π‡≠á‡¨®‡≠ç‡¨¶‡≠ç‡¨∞‡¨§‡¨®‡≠ü\", \"‡¨Ö‡¨∂‡≠ã‡¨ï\", \"‡¨ß‡¨∞‡¨ø‡¨§‡≠ç‡¨∞‡≠Ä\", \"‡¨∏‡¨æ‡¨Æ‡≠ç‡¨¨\", \"‡¨®‡≠Ä‡¨≥‡¨æ‡¨ö‡¨≥\", \"‡¨™‡¨ü‡¨ø‡¨Ü\", \"‡¨®‡≠ü‡¨æ‡¨™‡¨≤‡≠ç‡¨≤‡≠Ä\"]\n",
        "      self.compound_roots=compound_roots or [\n",
        "\n",
        "                                            \"‡¨∂‡¨ø‡¨ï‡≠ç‡¨∑‡¨æ\", \"‡¨®‡≠Ä‡¨§‡¨ø\", \"‡¨∏‡¨Ç‡¨∏‡≠ç‡¨ï‡≠É‡¨§‡¨ø\", \"‡¨ï‡≠á‡¨®‡≠ç‡¨¶‡≠ç‡¨∞\", \"‡¨¨‡¨ø‡¨≠‡¨æ‡¨ó\", \"‡¨™‡≠Å‡¨∏‡≠ç‡¨§‡¨ï\", \"‡¨Æ‡≠á‡¨≥‡¨æ\",\n",
        "    \"‡¨™‡≠ç‡¨∞‡¨∂‡¨æ‡¨∏‡¨®\", \"‡¨®‡¨ø‡≠ü‡¨Æ\", \"‡¨™‡¨∞‡≠Ä‡¨ï‡≠ç‡¨∑‡¨æ\", \"‡¨Æ‡¨®‡≠ç‡¨§‡≠ç‡¨∞‡¨£‡¨æ‡¨≥‡≠ü\", \"‡¨™‡≠ç‡¨∞‡¨ß‡¨æ‡¨®\", \"‡¨Æ‡¨®‡≠ç‡¨§‡≠ç‡¨∞‡≠Ä\",\n",
        "    \"‡¨¨‡¨ø‡¨∂‡≠ç‡≠±‡¨¨‡¨ø‡¨¶‡≠ç‡≠ü‡¨æ‡¨≥‡≠ü\", \"‡¨∏‡¨Ç‡¨∏‡≠ç‡¨•‡¨æ\", \"‡¨®‡¨ø‡≠ü‡≠ã‡¨ú‡¨®\", \"‡¨¨‡¨ø‡¨≠‡¨æ‡¨ó\", \"‡¨∏‡¨Æ‡¨ø‡¨§‡¨ø\", \"‡¨Ö‡¨ß‡¨ø‡¨ï‡¨æ‡¨∞‡≠Ä\",\n",
        "    \"‡¨ö‡¨ø‡¨ï‡¨ø‡¨§‡≠ç‡¨∏‡¨æ\", \"‡¨™‡≠ç‡¨∞‡¨§‡¨ø‡¨∑‡≠ç‡¨†‡¨æ‡¨®\", \"‡¨∂‡¨ø‡¨∂‡≠Å\", \"‡¨∏‡≠á‡¨¨‡¨æ\", \"‡¨â‡¨¶‡≠ç‡≠ü‡≠ã‡¨ó\", \"‡¨Ü‡≠ü‡≠ã‡¨ú‡¨®\",\n",
        "    \"‡¨§‡¨•‡≠ç‡≠ü\", \"‡¨™‡≠ç‡¨∞‡¨Ø‡≠Å‡¨ï‡≠ç‡¨§‡¨ø\", \"‡¨ñ‡≠á‡¨≥\", \"‡¨ï‡≠ç‡¨∞‡≠Ä‡¨°‡¨æ\", \"‡¨Ö‡¨≠‡¨ø‡¨Ø‡¨æ‡¨®\", \"‡¨Ö‡¨®‡≠Å‡¨∑‡≠ç‡¨†‡¨æ‡¨®\",\n",
        "    \"‡¨∏‡¨æ‡¨Ç‡¨∏‡≠ç‡¨ï‡≠É‡¨§‡¨ø‡¨ï\", \"‡¨¨‡¨ø‡¨ú‡≠ç‡¨û‡¨æ‡¨®\", \"‡¨§‡¨®‡≠ç‡¨§‡≠ç‡¨∞\", \"‡¨Ö‡¨®‡≠Å‡¨∏‡¨®‡≠ç‡¨ß‡¨æ‡¨®\", \"‡¨™‡≠ç‡¨∞‡¨ï‡¨æ‡¨∂‡¨®\",\n",
        "    \"‡¨¨‡≠ç‡≠ü‡¨¨‡¨∏‡≠ç‡¨•‡¨æ\", \"‡¨¨‡¨ú‡≠á‡¨ü\", \"‡¨â‡¨¶‡≠ç‚Äå‡¨ò‡¨æ‡¨ü‡¨®\", \"‡¨Ö‡¨ß‡¨ø‡¨¨‡≠á‡¨∂‡¨®\", \"‡¨¨‡≠à‡¨†‡¨ï\", \"‡¨â‡¨™‡¨∏‡≠ç‡¨•‡¨æ‡¨™‡¨®\",\n",
        "    \"‡¨®‡≠Ä‡¨§‡¨ø\", \"‡¨Ø‡≠ã‡¨ú‡¨®‡¨æ\", \"‡¨™‡¨¶‡¨ï‡≠ç‡¨∑‡≠á‡¨™\", \"‡¨™‡≠ç‡¨∞‡¨§‡¨ø‡¨¨‡≠á‡¨¶‡¨®\", \"‡¨∏‡¨Æ‡≠Ä‡¨ï‡≠ç‡¨∑‡¨æ\", \"‡¨∏‡≠Ç‡¨ö‡¨®‡¨æ\",\n",
        "    \"‡¨∏‡≠Å‡¨∞‡¨ï‡≠ç‡¨∑‡¨æ\", \"‡¨®‡¨ø‡¨∞‡≠ç‡¨¨‡¨æ‡¨ö‡¨®\", \"‡¨™‡≠ç‡¨∞‡¨ö‡¨æ‡¨∞\", \"‡¨®‡¨ø‡≠ü‡¨Æ‡¨æ‡¨¨‡¨≥‡≠Ä\", \"‡¨®‡¨ø‡≠ü‡¨Æ‡¨ï\", \"‡¨∏‡¨ö‡¨ø‡¨¨\",\n",
        "    \"‡¨™‡¨∞‡¨ø‡¨ö‡¨æ‡¨≥‡¨®‡¨æ\", \"‡¨™‡¨æ‡¨†‡≠ç‡≠ü‡¨ï‡≠ç‡¨∞‡¨Æ\", \"‡¨Ö‡¨®‡≠Å‡¨Æ‡≠ã‡¨¶‡¨®\", \"‡¨∂‡≠ç‡¨∞‡¨Æ\", \"‡¨â‡¨®‡≠ç‡¨®‡¨§‡¨ø\", \"‡¨Ö‡¨®‡≠Å‡¨∂‡¨æ‡¨∏‡¨®\"\n",
        "      ]\n",
        "      self.force_suffix_split=force_suffix_split\n",
        "      self.enable_fuzzy_split=enable_fuzzy_split\n",
        "\n",
        "    def normalize_unicode(self, text):\n",
        "      normalized_text=unicodedata.normalize(\"NFC\", text)\n",
        "      cleaned = normalized_text.replace('\\u200d', '').replace('\\u200c', '')\n",
        "      return cleaned\n",
        "\n",
        "\n",
        "    def is_known_token(self,token):\n",
        "      # sentencepiece tokenizer sometimes have \"__token\" structure in their vocab\n",
        "      return token in self.vocab or f\"__{token}\" in self.vocab\n",
        "\n",
        "    def get_graphemes(self,word):\n",
        "      return regex.findall(r'\\X', word)\n",
        "\n",
        "    def is_named_entity(self, word):\n",
        "      for entity in self.named_entities:\n",
        "          if self.enable_fuzzy_split:\n",
        "              if word.startswith(entity) or word == entity:\n",
        "                  return True\n",
        "          else:\n",
        "              if word == entity:\n",
        "                  return True\n",
        "      return False\n",
        "\n",
        "    def compound_splitter(self,word):\n",
        "      graphemes=self.get_graphemes(word)\n",
        "      for i in range(2,len(graphemes)):\n",
        "        left=\"\".join(graphemes[:i])\n",
        "        right=\"\".join(graphemes[i:])\n",
        "\n",
        "        if left in self.compound_roots and right in self.compound_roots:\n",
        "          # print(f\"Compound root split: '{word}' ‚Üí '{left}' + '{right}'\")\n",
        "          return f\"{left} {right}\"\n",
        "      return word\n",
        "\n",
        "    def split_hyphenated(self, word):\n",
        "      if \"-\" in word:\n",
        "          parts = word.split(\"-\")\n",
        "          if len(parts) == 2 and all(len(p) > 1 for p in parts):\n",
        "              # print(f\"üî® Hyphen split: '{word}' ‚Üí '{parts[0]} {parts[1]}'\")\n",
        "              return f\"{parts[0]} {parts[1]}\"\n",
        "      return word\n",
        "\n",
        "    def split_suffix(self, word, depth=0):\n",
        "        indent = \"  \" * depth\n",
        "        graphemes = self.get_graphemes(word)\n",
        "\n",
        "        for i in range(1, len(graphemes)):\n",
        "            stem = \"\".join(graphemes[:i])\n",
        "            suffix = \"\".join(graphemes[i:])\n",
        "\n",
        "            if suffix in self.suffixes and len(stem) > 1:\n",
        "              # disable unknown token check on an ad hoc basis\n",
        "                if self.force_suffix_split or  not self.is_known_token(stem) or not self.is_known_token(suffix):\n",
        "\n",
        "                    # print(f\"{indent}üìç Split (grapheme-safe): '{word}' ‚Üí '{stem}' + '{suffix}'\")\n",
        "                    split_stem = self.split_suffix(stem, depth + 1)\n",
        "                    return f\"{split_stem} {suffix}\"\n",
        "                else:\n",
        "                    print(f\"{indent}‚ö†Ô∏è Not splitting: '{word}' ‚Üí parts unknown\")\n",
        "        print(f\"{indent}‚úÖ Final stem: '{word}'\")\n",
        "        return word\n",
        "\n",
        "\n",
        "    def preprocess_text(self,text):\n",
        "      normalized=self.normalize_unicode(text)\n",
        "      words=normalized.split()\n",
        "\n",
        "      processed=[]\n",
        "      for word in words:\n",
        "\n",
        "        if self.is_named_entity(word):\n",
        "            print(f\"Skipping named entity(fuzzy): {word}\")\n",
        "            processed.append(word)\n",
        "        else:\n",
        "            word = self.split_hyphenated(word)\n",
        "            word = self.compound_splitter(word)\n",
        "            word = self.split_suffix(word)\n",
        "            processed.append(word)\n",
        "      return \" \".join(processed)\n",
        "\n",
        "    def visualize_token_split(self, text, return_stats=False):\n",
        "      print(\"\\nüß™ ORIGINAL INPUT\")\n",
        "      print(\"üìå Text:\", text)\n",
        "\n",
        "      # --- Raw tokenization ---\n",
        "      raw_tokens = self.tokenizer.tokenize(text)\n",
        "      print(\"\\nüß© RAW TOKENIZATION\")\n",
        "      print(f\"üß± Tokens ({len(raw_tokens)}):\", raw_tokens)\n",
        "\n",
        "      # --- Preprocessing ---\n",
        "      split_text = self.preprocess_text(text)\n",
        "\n",
        "      # --- Preprocessed tokenization ---\n",
        "      pre_tokens = self.tokenizer.tokenize(split_text)\n",
        "      print(\"\\nüõ†Ô∏è AFTER PREPROCESSING\")\n",
        "      print(\"üßº Cleaned:\", split_text)\n",
        "      print(f\"üî§ Tokens ({len(pre_tokens)}):\", pre_tokens)\n",
        "\n",
        "      # --- Comparison ---\n",
        "      print(\"\\nüìä TOKEN COUNT COMPARISON\")\n",
        "      delta = len(raw_tokens) - len(pre_tokens)\n",
        "      if delta > 0:\n",
        "          print(f\"‚úÖ Token count reduced by: {delta} tokens\")\n",
        "      elif delta < 0:\n",
        "          print(f\"‚ö†Ô∏è Token count increased by: {-delta} tokens\")\n",
        "      else:\n",
        "          print(\"‚ûñ Token count unchanged\")\n",
        "\n",
        "      print(\"‚Äî\" * 50)\n",
        "\n",
        "      if return_stats:\n",
        "          return {\n",
        "              \"original\": text,\n",
        "              \"preprocessed\": split_text,\n",
        "              \"raw_tokens\": raw_tokens,\n",
        "              \"pre_tokens\": pre_tokens,\n",
        "              \"delta\": delta\n",
        "          }\n",
        "\n",
        "    def encode(self, text, **kwargs):\n",
        "      preprocessed=self.preprocess_text(text)\n",
        "      encoded=self.tokenizer.encode(preprocessed, **kwargs)\n",
        "      return encoded\n"
      ],
      "metadata": {
        "id": "HjnTqg2IsPIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O77ofDAWlcoQ",
        "outputId": "91bbd7ca-5c9a-4395-ee25-a4287f28fe60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clean the dataframe fields for odia and english\n",
        "import re\n",
        "def clean_english(text):\n",
        "  if not isinstance(text,str) or pd.isna(text):\n",
        "    return \"\"\n",
        "\n",
        "  text=unicodedata.normalize('NFKC', text)\n",
        "\n",
        "  text=re.sub(r'\\s+',' ',  text)\n",
        "      # Remove URLs\n",
        "  text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "  # Remove HTML tags\n",
        "  text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "  # Standardize punctuation spacing\n",
        "  text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "vvjXAUW0nzz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "odia_tokenizer=OdiaTokenizerWrapper(base_tokenizer=base_mistral_tokenizer)\n",
        "def preprocess_dataframe(df):\n",
        "  df_clean=df.copy()\n",
        "\n",
        "  df_clean['eng_string_clean']=df_clean['eng_string'].apply(clean_english)\n",
        "  df_clean['or_string_clean']=df_clean['or_string'].apply(odia_tokenizer.preprocess_text)\n",
        "\n",
        "  df_clean=df_clean.reset_index(drop=True)\n",
        "  return df_clean"
      ],
      "metadata": {
        "id": "dB4TyE0R5oR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/drive/My Drive/bpcc_eng_odia.csv')\n",
        "cleaned_df=preprocess_dataframe(df)"
      ],
      "metadata": {
        "id": "SU77_veE8gLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def convert_to_huggingface_dataset(df, test_size=0.1, val_size=0.1, random_state=42):\n",
        "  df_copy=df.copy()\n",
        "\n",
        "  train_df,test_df=train_test_split(df_copy, test_size=test_size, random_state=random_state)\n",
        "  train_df,val_df=train_test_split(train_df, test_size=test_size, random_state=random_state)\n",
        "\n",
        "  dataset_dict=DatasetDict({\n",
        "      \"train\":Dataset.from_pandas(train_df),\n",
        "      \"validation\":Dataset.from_pandas(val_df),\n",
        "      \"test\":Dataset.from_pandas(test_df)\n",
        "  })\n",
        "\n",
        "  return dataset_dict\n"
      ],
      "metadata": {
        "id": "od50Epfbj34n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_dataset=convert_to_huggingface_dataset(cleaned_df)"
      ],
      "metadata": {
        "id": "F5M12If72A_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHZst5XY7Yql",
        "outputId": "c05a2a4f-db71-421f-8152-3ee8d49b2ed4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['eng_string', 'or_string', 'eng_string_clean', 'or_string_clean', '__index_level_0__'],\n",
              "        num_rows: 73256\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['eng_string', 'or_string', 'eng_string_clean', 'or_string_clean', '__index_level_0__'],\n",
              "        num_rows: 8140\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['eng_string', 'or_string', 'eng_string_clean', 'or_string_clean', '__index_level_0__'],\n",
              "        num_rows: 9044\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# format for finetuning translation\n",
        "import json\n",
        "def format_for_translation(example, template_format=\"chatml\"):\n",
        "  eng_text=example['eng_string_clean']\n",
        "  or_text=example['or_string_clean']\n",
        "  if template_format==\"chatml\":\n",
        "    formatted={\n",
        "         \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful translation assistant that translates English to Odia.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Translate the following English text to Odia: {eng_text}\"},\n",
        "                {\"role\": \"assistant\", \"content\": or_text}\n",
        "            ]\n",
        "    }\n",
        "  elif template_format==\"alpaca\":\n",
        "        # Alpaca format\n",
        "        formatted = {\n",
        "            \"instruction\": \"Translate the following English text to Odia.\",\n",
        "            \"input\": eng_text,\n",
        "            \"output\": or_text\n",
        "        }\n",
        "  elif template_format==\"llama2\":\n",
        "    formatted={\n",
        "        \"text\": f\"<s>[INST] <<SYS>>\\nYou are a helpful translation assistant that translates English to Odia.\\n<</SYS>>\\n\\nTranslate the following English text to Odia: {eng_text} [/INST] {or_text}</s>\"\n",
        "    }\n",
        "  elif template_format==\"text_only\":\n",
        "    formatted={\n",
        "         \"text\": f\"English: {eng_text}\\nOdia: {or_text}\"\n",
        "    }\n",
        "\n",
        "  return formatted\n",
        "\n",
        "\n",
        "def convert_to_jsonl(dataset, output_path, template_format=\"chatml\"):\n",
        "  with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    for example in dataset:\n",
        "      # print(row['eng_string_clean'], row['or_string_clean'])\n",
        "      formatted=format_for_translation(example, template_format)\n",
        "      f.write(json.dumps(formatted, ensure_ascii=False)+'\\n')\n",
        "  print(f\"Saved {len(dataset)} examples to {output_path}\")"
      ],
      "metadata": {
        "id": "q65p0pYO9S3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def prepare_all_formats(dataset_dict, output_dir=\"/content/drive/My Drive/formatted_data\"):\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "  format_options=[\"chatml\", \"alpaca\", \"llama2\", \"text_only\"]\n",
        "\n",
        "  for split_name, dataset in dataset_dict.items():\n",
        "    for fmt in format_options:\n",
        "      output_path=f\"{output_dir}/{split_name}_{fmt}.jsonl\"\n",
        "      convert_to_jsonl(dataset, output_path, template_format=fmt)\n",
        "  print(f\"All dataset formats saved to {output_dir}\")\n"
      ],
      "metadata": {
        "id": "q0lVJekh-nXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_all_formats(hf_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKZRac8lLFXC",
        "outputId": "753ac7d9-1188-4ca7-ea6b-3678068205bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 73256 examples to /content/drive/My Drive/formatted_data/train_chatml.jsonl\n",
            "Saved 73256 examples to /content/drive/My Drive/formatted_data/train_alpaca.jsonl\n",
            "Saved 73256 examples to /content/drive/My Drive/formatted_data/train_llama2.jsonl\n",
            "Saved 73256 examples to /content/drive/My Drive/formatted_data/train_text_only.jsonl\n",
            "Saved 8140 examples to /content/drive/My Drive/formatted_data/validation_chatml.jsonl\n",
            "Saved 8140 examples to /content/drive/My Drive/formatted_data/validation_alpaca.jsonl\n",
            "Saved 8140 examples to /content/drive/My Drive/formatted_data/validation_llama2.jsonl\n",
            "Saved 8140 examples to /content/drive/My Drive/formatted_data/validation_text_only.jsonl\n",
            "Saved 9044 examples to /content/drive/My Drive/formatted_data/test_chatml.jsonl\n",
            "Saved 9044 examples to /content/drive/My Drive/formatted_data/test_alpaca.jsonl\n",
            "Saved 9044 examples to /content/drive/My Drive/formatted_data/test_llama2.jsonl\n",
            "Saved 9044 examples to /content/drive/My Drive/formatted_data/test_text_only.jsonl\n",
            "All dataset formats saved to /content/drive/My Drive/formatted_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3HS-gqpw4C_",
        "outputId": "1e8833d0-0e9f-41e0-9802-99ea9118c29a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.4)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK8Q4EQvVHlf",
        "outputId": "8e8e72a1-a736-461f-f761-7a9175f3772a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.50.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing the instructions\n",
        "def preprocess_function(examples,tokenizer, max_length=512):\n",
        "  # tokenize inputs and targets\n",
        "  if \"text\" in examples:\n",
        "    texts=examples[\"text\"]\n",
        "    model_inputs=tokenizer(texts, max_length=max_length, truncation=True)\n",
        "    return model_inputs\n",
        "\n",
        "  else:\n",
        "    model_inputs={}\n",
        "    for i in range(len(examples[\"instruction\"])):\n",
        "      text = f\"### Instruction: {examples['instruction'][i]}\\n\"\n",
        "      text += f\"### Input: {examples['input'][i]}\\n\"\n",
        "      text += f\"### Response: {examples['output'][i]}\"\n",
        "      # tokenize the dataset\n",
        "      tokenized=tokenizer(text, max_length=max_length, truncation=True)\n",
        "\n",
        "      # Add to batch\n",
        "      for k, v in tokenized.items():\n",
        "        if k not in model_inputs:\n",
        "          model_inputs[k]=[]\n",
        "        model_inputs[k].append(v)\n",
        "    return model_inputs\n"
      ],
      "metadata": {
        "id": "4uh1ZTf5XSbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "\n",
        "from datasets import load_dataset\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    level=logging.INFO\n",
        ")\n",
        "\n",
        "# core SFT logic block\n",
        "def train_translation_model(\n",
        "    model_name,\n",
        "    dataset_path,\n",
        "    output_dir,\n",
        "    format_type=\"llama2\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=2e-04,\n",
        "    bf16=True,\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    eval_steps=200,\n",
        "    lora_r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    use_peft=True\n",
        "):\n",
        "  logging.info(\"Creating output directory at %s\", output_dir)\n",
        "  os.makedirs(output_dir,exist_ok=True)\n",
        "\n",
        "  # load tokenizer\n",
        "  logging.info(\"Loading tokeniser from %s\",model_name)\n",
        "  tokenizer=AutoTokenizer.from_pretrained(model_name, padding_side=\"right\",use_fast=False)\n",
        "  if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token=tokenizer.eos_token\n",
        "\n",
        "\n",
        "  # load the model for full fine tuning\n",
        "  logging.info(\"Loading model from %s\", model_name)\n",
        "  if bf16 and torch.cuda.is_available():\n",
        "    bnb_config=BitsAndBytesConfig(load_in_8bit=True)\n",
        "    model_output=AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        quantization_config=bnb_config,\n",
        "        output_loading_info=True\n",
        "    )\n",
        "    if isinstance(model_output, tuple):\n",
        "      model=model_output[0]\n",
        "    else:\n",
        "      model=model_output\n",
        "  else:\n",
        "    model_output=AutoModelForCausalLM.from_pretrained(model_name,trust_remote_code=True, output_loading_info=True)\n",
        "    if isinstance(model_output, tuple):\n",
        "      model = model_output[0]\n",
        "    else:\n",
        "      model = model_output\n",
        "\n",
        "  if use_peft:\n",
        "    from peft import LoraConfig, get_peft_model, TaskType\n",
        "    logging.info(\"Applying LoRA config for PEFT\")\n",
        "    lora_config=LoraConfig(\n",
        "        r=lora_r,\n",
        "        lora_alpha=lora_alpha,\n",
        "        lora_dropout=lora_dropout,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"\n",
        "                    , \"down_proj\", \"lm_head\"]\n",
        "\n",
        "    )\n",
        "    model=get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "  # load the dataset from Google drive\n",
        "  logging.info(\"Loading data from %s\", dataset_path)\n",
        "  dataset=load_dataset(\"json\",data_files=dataset_path)\n",
        "\n",
        "  # if no explicit validation sets are provided in the loaded data\n",
        "  if \"validation\" not in dataset:\n",
        "    logging.info(\"Splitting dataset into training and validation sets\")\n",
        "    dataset=dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "\n",
        "  # preprocess the datarset\n",
        "  logging.info(\"Preprocessing the dataset\")\n",
        "  tokenized_dataset=dataset.map(\n",
        "      lambda examples: preprocess_function(examples, tokenizer),\n",
        "      batched=True,\n",
        "      remove_columns=dataset[\"train\"].column_names\n",
        "  )\n",
        "\n",
        "  # prepare the data collator\n",
        "  logging.info(\"Prepare the data collator\")\n",
        "  data_collator=DataCollatorForLanguageModeling(\n",
        "      tokenizer=tokenizer,\n",
        "      mlm=False\n",
        "\n",
        "  )\n",
        "  # training args. Note the seq2seq models are probably not supported for Mistral class of models.\n",
        "  # hence going for normal trainers\n",
        "  logging.info(\"Setting up training arguments\")\n",
        "  training_args=TrainingArguments(\n",
        "      output_dir=output_dir,\n",
        "      num_train_epochs=num_train_epochs,\n",
        "      per_device_train_batch_size=per_device_train_batch_size,\n",
        "      per_device_eval_batch_size=per_device_train_batch_size,\n",
        "      gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "      learning_rate=learning_rate,\n",
        "      weight_decay=0.01,\n",
        "      bf16=bf16,\n",
        "      logging_steps=logging_steps,\n",
        "      save_steps=save_steps,\n",
        "      eval_steps=eval_steps,\n",
        "      save_total_limit=3,\n",
        "      evaluation_strategy=\"steps\",\n",
        "      load_best_model_at_end=True,\n",
        "      report_to=\"tensorboard\",\n",
        "      push_to_hub=False,\n",
        "      gradient_checkpointing=True\n",
        "  )\n",
        "\n",
        "  # init trainer\n",
        "  trainer=Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=tokenized_dataset[\"train\"],\n",
        "      eval_dataset=tokenized_dataset[\"validation\"] if \"validation\" in tokenized_dataset else tokenized_dataset[\"test\"],\n",
        "      tokenizer=tokenizer,\n",
        "      data_collator=data_collator\n",
        "  )\n",
        "   # Start training\n",
        "  logging.info(\"Starting training\")\n",
        "  trainer.train()\n",
        "  logging.info(\"Training completed\")\n",
        "\n",
        "  # Save the fine-tuned model and tokenizer\n",
        "  logging.info(\"Saving the model and tokenizer to %s\", output_dir)\n",
        "  trainer.save_model(output_dir)\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "  logging.info(\"Model and tokenizer saved successfully\")\n",
        "\n",
        "  return model, tokenizer\n",
        "\n"
      ],
      "metadata": {
        "id": "XLXZSILC_B3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Hard-coded parameters for Google Colab environment\n",
        "    model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"  # Change this if you prefer another model\n",
        "    # Set this to the path of your JSONL formatted dataset in Google Drive\n",
        "    dataset_path = \"/content/drive/My Drive/formatted_data/train_llama2.jsonl\"\n",
        "    # Set the output directory in your Google Drive\n",
        "    output_dir = \"/content/drive/My Drive/mistral-7b-ft-eng-or-translation\"\n",
        "    format_type = \"llama2\"\n",
        "    num_train_epochs = 3\n",
        "    per_device_train_batch_size = 4\n",
        "    gradient_accumulation_steps = 16\n",
        "    learning_rate = 2e-4\n",
        "    bf16 = True\n",
        "    logging_steps = 10\n",
        "    save_steps = 200\n",
        "    eval_steps = 200\n",
        "\n",
        "    logging.info(\"Starting full fine-tuning with the following parameters:\")\n",
        "    logging.info(\"Model: %s\", model_name)\n",
        "    logging.info(\"Dataset Path: %s\", dataset_path)\n",
        "    logging.info(\"Output Directory: %s\", output_dir)\n",
        "\n",
        "    model,tokenizer=train_translation_model(\n",
        "        model_name=model_name,\n",
        "        dataset_path=dataset_path,\n",
        "        output_dir=output_dir,\n",
        "        format_type=format_type,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        learning_rate=learning_rate,\n",
        "        bf16=bf16,\n",
        "        logging_steps=logging_steps,\n",
        "        save_steps=save_steps,\n",
        "        eval_steps=eval_steps,\n",
        "\n",
        "    )"
      ],
      "metadata": {
        "id": "Kzjcf8mcr0h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "atDxm64OrOtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "def merge_lora_model(model_path, output_path, base_model_name):\n",
        "    \"\"\"\n",
        "    Loads the LoRA‚Äìfinetuned model, merges the LoRA weights into the base model,\n",
        "    and saves the merged model.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the directory of the LoRA‚Äìfinetuned model.\n",
        "        output_path (str): Directory to save the merged model.\n",
        "        base_model_name (str): Name or path of the base model (used for initialization).\n",
        "    \"\"\"\n",
        "    # Load the base model\n",
        "    print(f\"Loading base model {base_model_name} ...\")\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Load the fine-tuned LoRA model.\n",
        "    print(f\"Loading LoRA model from {model_path} ...\")\n",
        "    model = PeftModel.from_pretrained(base_model, model_path)\n",
        "\n",
        "    # Merge the LoRA weights into the base model.\n",
        "    print(\"Merging LoRA weights into the base model ...\")\n",
        "    model = model.merge_and_unload()\n",
        "\n",
        "    # Save the merged model to the specified output directory.\n",
        "    print(f\"Saving the merged model to {output_path} ...\")\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    model.save_pretrained(output_path)\n",
        "    model.push_to_hub(\"Prajna1999/mistral-7b-translation-finetuned\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    tokenizer.save_pretrained(output_path)\n",
        "    tokenizer.push_to_hub(\"Prajna1999/mistral-7b-translation-finetuned\")\n",
        "    print(f\"Merged model saved at {output_path}\")"
      ],
      "metadata": {
        "id": "gK-F-TC4O9Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Hard-coded parameters for Google Colab\n",
        "    base_model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"  # Change as needed\n",
        "    model_path = \"/content/drive/My Drive/mistral-7b-ft-eng-or-translation\"  # LoRA‚Äìfinetuned model path\n",
        "    output_path = \"/content/drive/My Drive/mistral-7b-ft-eng-or-translation-merged-model\"  # Path to save the merged model\n",
        "\n",
        "    merge_lora_model(model_path, output_path, base_model_name)"
      ],
      "metadata": {
        "id": "aCd6dKDr-xPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "ELr2Edla79Rt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}